---
title: 'Tree based Method: Random Forest'
author: R package build
date: '2021-07-29'
slug: tree-based-method-random-forest
categories:
  - tidymodels
  - ggplot2
  - Basics
  - Tidyverse
tags: []
---
## Overview

To implement Random Forest method, the bank data set is used here, on which dataset, the classification technique is used earlier. As we know, Random Forest being one of the supervised machine learning, provides more flexibility and accuracy of the model over decision trees adding extra randomness. 

In Random Forests the idea is to decorrelate the several trees which are generated by the different bootstrapped samples from training Data. The idea is to build lots of Trees in such a way to make the Correlation between the Trees smaller.

```{r echo = FALSE, message=FALSE, warning=FALSE}
library(tidymodels)
library(ggplot2)
```
## Revisiting the Dataset

To classify whether a customer will subscribe to the term deposit or not based on the predictors given below, we will apply Random forest method of supervised learning to train trees on Bootstrap samples.
```{r echo = FALSE, message=FALSE, warning=FALSE}
bank <- read.table("bank.csv", sep=";", header=T)
head(bank)
```

## Spliting the Data

The Data is spitted into two groups of training and test data with 70 and 30 proportions.For fitting the Random Forest all the Predictors are used in the dataset. 500 Trees with 4 mtry and 2 min_n are fitted where mtry is a hyper-parameter of the random forest model that determines how many variables the model uses to split the trees or the predictors seen at each node, measured by square root of number predictors and min_n is the smallest node size allowed.  
```{r echo = FALSE, message=FALSE, warning=FALSE}
set.seed(123)
bank_split <- initial_split(bank, prop = 0.70, strata=y)
bank_train <- training(bank_split)
bank_test  <- testing(bank_split)

```
To know the train and test data for bank data set, the counts and proportions are measured below:
```{r echo = FALSE, message=FALSE, warning=FALSE}
#counts_bank_train <- table(bank_train$y)
#counts_bank_train
#prop_yes_bank_train <- counts_bank_train["yes"]/
 #                 sum(counts_bank_train)
#prop_yes_bank_train

#counts_bank_test <- table(bank_test$y)
#counts_bank_test
#prop_yes_bank_test <- counts_bank_test["yes"]/
 #                 sum(counts_bank_test)
#prop_yes_bank_test
```

```{r echo = FALSE, message=FALSE, warning=FALSE}
bank_train$y <- ifelse(bank_train$y == "yes", 1, 0)
bank_train$y <- factor(bank_train$y, levels = c(0, 1))
head(bank_train)
```
## Train the model

Here the random forest is used to evaluate the classification model with the train dataset. We see that, the prediction error has come out as 7% approximately, which means the number of wrongly classifying the OBB sample is low. So, we can assume that the model will give low variance.Here,the model uses 4 variables to split the trees and 500 trees, to find the final value of hyper-parameter of the random forest model, we can try using different trees and mtry.In this case, using different number of trees giving almost same result.(I have tried changing the number to tress up to 2000) 
```{r echo = FALSE, message=FALSE, warning=FALSE}
library(tidymodels)
library(vip)
# Specify a random forest
spec <- rand_forest(mtry = 4, trees = 500, min_n = 2) %>%
	set_mode("classification") %>%
    set_engine("ranger", importance = "impurity")

# Train the forest
model <- spec %>%
    fit(y ~ ., data = bank_train)
model
```
## Importance of Variable
```{r echo = FALSE, message=FALSE, warning=FALSE}
# Plot the variable importance
vip::vip(model)
```
Random forests is used to rank the importance of variables in our classification problem. From the above figure, we find that duration is the most important variable with age, balance and day etc where the feature education is less significant. This gives a clear idea about taking the decision for a customer subscription in case of term deposit.

## Predict the model

Now to make predictions the test dataset is used using the predictions function and we see that a new column has added named '.pred_class'. This prediction is made based on the all the dependent variables. As we know now variable importance, based on that, we can use the most significant variables.
```{r echo = FALSE, message=FALSE, warning=FALSE}
bank_test$y <- ifelse(bank_test$y == "yes", 1, 0)
bank_test$y <- factor(bank_test$y, levels = c(0, 1))
#head(bank_test)
predictions <- predict(model, new_data = bank_test) %>%
  bind_cols(bank_test) 
predictions
```
In the new column, we can see the “y” values (either 0 or 1) for each customer to subscribe the term deposit or not.

## Evaluate the model

Below the prediction is used to compute the confusion matrix and see the accuracy score of the model.As we know,the confusion matrix is also known as the error matrix that shows the visualization of the performance of the classification model.
```{r echo = FALSE, message=FALSE, warning=FALSE}
conf_mat(predictions,
     estimate = .pred_class,
     truth = y)
```
From the above confusion matrix, it is clear that, the True Positive value is calculated as 48 that is the prediction of term deposit subscription is a yes and it actually is yes.The True Negative is 1161, predicted negative and it is true, that is,the term deposit subscription is a no and it actually is no as predicted.False Positive or Type 1 Error here is 39 that is, predicted positive and it is false.So, the term deposit subscription is a yes and it actually is no.False Negative or Type 2 Error is 109, predicted negative and it is false, that is, the term deposit subscription is a no and it actually is yes fo a customer.
```{r echo = FALSE, message=FALSE, warning=FALSE}
accuracy(predictions,
     estimate = .pred_class,
     truth = y)
```
The accuracy from the confusion matrix here is 89%.This value of accuracy 89% means that identification of 1 of every 10 subscription of yes is incorrect, and 9 is correct.

## Plot the model

Now to plot the model here we fit the random forest again without specifying the trees and hyper-parameters
as follows:
```{r echo = FALSE, message=FALSE, warning=FALSE}
require(randomForest)
model <- randomForest(y ~ . , data = bank_train)
model
plot(model)
```
After executing the above code, the output is produced that shows the number of decision trees developed using the classification model for random forest algorithms, i.e. 500 decision trees. Here the error are plotted against the trees and we see that error decreased a bit with the increase of number of trees.

## Summary

We can conclude saying that the random forest method gives the more accurate result as it gives 89% accuracy of the model that minimize the correlation between the variables with adding extra trees and the chance of over fitting is also avoided. 



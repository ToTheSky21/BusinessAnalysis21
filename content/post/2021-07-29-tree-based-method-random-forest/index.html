---
title: 'Tree based Method: Random Forest'
author: R package build
date: '2021-07-29'
slug: tree-based-method-random-forest
categories:
  - tidymodels
  - ggplot2
  - Basics
  - Tidyverse
tags: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="overview" class="section level2">
<h2>Overview</h2>
<p>To implement Random Forest method, the bank data set is used here, on which dataset, the classification technique is used earlier. As we know, Random Forest being one of the supervised machine learning, provides more flexibility and accuracy of the model over decision trees adding extra randomness.</p>
<p>In Random Forests the idea is to decorrelate the several trees which are generated by the different bootstrapped samples from training Data. The idea is to build lots of Trees in such a way to make the Correlation between the Trees smaller.</p>
</div>
<div id="revisiting-the-dataset" class="section level2">
<h2>Revisiting the Dataset</h2>
<p>To classify whether a customer will subscribe to the term deposit or not based on the predictors given below, we will apply Random forest method of supervised learning to train trees on Bootstrap samples.</p>
<pre><code>##   age         job marital education default balance housing loan  contact day
## 1  30  unemployed married   primary      no    1787      no   no cellular  19
## 2  33    services married secondary      no    4789     yes  yes cellular  11
## 3  35  management  single  tertiary      no    1350     yes   no cellular  16
## 4  30  management married  tertiary      no    1476     yes  yes  unknown   3
## 5  59 blue-collar married secondary      no       0     yes   no  unknown   5
## 6  35  management  single  tertiary      no     747      no   no cellular  23
##   month duration campaign pdays previous poutcome  y
## 1   oct       79        1    -1        0  unknown no
## 2   may      220        1   339        4  failure no
## 3   apr      185        1   330        1  failure no
## 4   jun      199        4    -1        0  unknown no
## 5   may      226        1    -1        0  unknown no
## 6   feb      141        2   176        3  failure no</code></pre>
</div>
<div id="spliting-the-data" class="section level2">
<h2>Spliting the Data</h2>
<p>The Data is spitted into two groups of training and test data with 70 and 30 proportions.For fitting the Random Forest all the Predictors are used in the dataset. 500 Trees with 4 mtry and 2 min_n are fitted where mtry is a hyper-parameter of the random forest model that determines how many variables the model uses to split the trees or the predictors seen at each node, measured by square root of number predictors and min_n is the smallest node size allowed.</p>
<p>To know the train and test data for bank data set, the counts and proportions are measured below:</p>
<pre><code>##   age           job marital education default balance housing loan  contact day
## 1  30    unemployed married   primary      no    1787      no   no cellular  19
## 2  33      services married secondary      no    4789     yes  yes cellular  11
## 4  30    management married  tertiary      no    1476     yes  yes  unknown   3
## 5  59   blue-collar married secondary      no       0     yes   no  unknown   5
## 7  36 self-employed married  tertiary      no     307     yes   no cellular  14
## 8  39    technician married secondary      no     147     yes   no cellular   6
##   month duration campaign pdays previous poutcome y
## 1   oct       79        1    -1        0  unknown 0
## 2   may      220        1   339        4  failure 0
## 4   jun      199        4    -1        0  unknown 0
## 5   may      226        1    -1        0  unknown 0
## 7   may      341        1   330        2    other 0
## 8   may      151        2    -1        0  unknown 0</code></pre>
</div>
<div id="train-the-model" class="section level2">
<h2>Train the model</h2>
<p>Here the random forest is used to evaluate the classification model with the train dataset. We see that, the prediction error has come out as 7% approximately, which means the number of wrongly classifying the OBB sample is low. So, we can assume that the model will give low variance.Here,the model uses 4 variables to split the trees and 500 trees, to find the final value of hyper-parameter of the random forest model, we can try using different trees and mtry.In this case, using different number of trees giving almost same result.(I have tried changing the number to tress up to 2000)</p>
<pre><code>## parsnip model object
## 
## Fit time:  1.3s 
## Ranger result
## 
## Call:
##  ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), num.trees = ~500, min.node.size = min_rows(~2, x), importance = ~&quot;impurity&quot;,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) 
## 
## Type:                             Probability estimation 
## Number of trees:                  500 
## Sample size:                      3164 
## Number of independent variables:  16 
## Mtry:                             4 
## Target node size:                 2 
## Variable importance mode:         impurity 
## Splitrule:                        gini 
## OOB prediction error (Brier s.):  0.07068625</code></pre>
</div>
<div id="importance-of-variable" class="section level2">
<h2>Importance of Variable</h2>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" width="672" />
Random forests is used to rank the importance of variables in our classification problem. From the above figure, we find that duration is the most important variable with age, balance and day etc where the feature education is less significant. This gives a clear idea about taking the decision for a customer subscription in case of term deposit.</p>
</div>
<div id="predict-the-model" class="section level2">
<h2>Predict the model</h2>
<p>Now to make predictions the test dataset is used using the predictions function and we see that a new column has added named ‘.pred_class’. This prediction is made based on the all the dependent variables. As we know now variable importance, based on that, we can use the most significant variables.</p>
<pre><code>## # A tibble: 1,357 x 18
##    .pred_class   age job         marital education default balance housing loan 
##    &lt;fct&gt;       &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;
##  1 0              35 management  single  tertiary  no         1350 yes     no   
##  2 0              35 management  single  tertiary  no          747 no      no   
##  3 0              31 blue-collar married secondary no          360 yes     yes  
##  4 0              44 services    single  secondary no          106 no      no   
##  5 1              32 management  single  tertiary  no         2536 yes     no   
##  6 0              61 admin.      married unknown   no         4629 yes     no   
##  7 1              37 technician  single  secondary no          228 yes     no   
##  8 0              57 management  married secondary no           82 no      yes  
##  9 0              54 technician  divorc… secondary no          784 yes     yes  
## 10 0              48 management  married tertiary  no         5887 no      no   
## # … with 1,347 more rows, and 9 more variables: contact &lt;chr&gt;, day &lt;int&gt;,
## #   month &lt;chr&gt;, duration &lt;int&gt;, campaign &lt;int&gt;, pdays &lt;int&gt;, previous &lt;int&gt;,
## #   poutcome &lt;chr&gt;, y &lt;fct&gt;</code></pre>
<p>In the new column, we can see the “y” values (either 0 or 1) for each customer to subscribe the term deposit or not.</p>
</div>
<div id="evaluate-the-model" class="section level2">
<h2>Evaluate the model</h2>
<p>Below the prediction is used to compute the confusion matrix and see the accuracy score of the model.As we know,the confusion matrix is also known as the error matrix that shows the visualization of the performance of the classification model.</p>
<pre><code>##           Truth
## Prediction    0    1
##          0 1161  109
##          1   39   48</code></pre>
<p>From the above confusion matrix, it is clear that, the True Positive value is calculated as 48 that is the prediction of term deposit subscription is a yes and it actually is yes.The True Negative is 1161, predicted negative and it is true, that is,the term deposit subscription is a no and it actually is no as predicted.False Positive or Type 1 Error here is 39 that is, predicted positive and it is false.So, the term deposit subscription is a yes and it actually is no.False Negative or Type 2 Error is 109, predicted negative and it is false, that is, the term deposit subscription is a no and it actually is yes fo a customer.</p>
<pre><code>## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.891</code></pre>
<p>The accuracy from the confusion matrix here is 89%.This value of accuracy 89% means that identification of 1 of every 10 subscription of yes is incorrect, and 9 is correct.</p>
</div>
<div id="plot-the-model" class="section level2">
<h2>Plot the model</h2>
<p>Now to plot the model here we fit the random forest again without specifying the trees and hyper-parameters
as follows:</p>
<pre><code>## 
## Call:
##  randomForest(formula = y ~ ., data = bank_train) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 4
## 
##         OOB estimate of  error rate: 10.3%
## Confusion matrix:
##      0   1 class.error
## 0 2732  68  0.02428571
## 1  258 106  0.70879121</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-11-1.png" width="672" />
After executing the above code, the output is produced that shows the number of decision trees developed using the classification model for random forest algorithms, i.e. 500 decision trees. Here the error are plotted against the trees and we see that error decreased a bit with the increase of number of trees.</p>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>We can conclude saying that the random forest method gives the more accurate result as it gives 89% accuracy of the model that minimize the correlation between the variables with adding extra trees and the chance of over fitting is also avoided.</p>
</div>
